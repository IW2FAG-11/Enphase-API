#!/usr/bin/env python
# -*- coding: utf-8 -*-

# This file is part of Enphase-API <https://github.com/Matthew1471/Enphase-API>
# Copyright (C) 2023 Matthew1471!
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License version 3 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

import json     # This script makes heavy use of JSON parsing.
import os.path  # We check whether a file exists and manipulate filepaths.

# All the shared Enphase® functions are in these packages.
from enphase_api.cloud.authentication import Authentication
from enphase_api.local.gateway import Gateway

# Enable this mode to perform no actual requests.
TEST_ONLY = False

# This script's version.
VERSION = 0.1

def get_header_section(endpoint, file_depth=0):
    # Heading.
    result = '= ' + endpoint['name'] + '\n'

    # Table of Contents.
    result += ':toc: preamble\n'

    # Reference.
    result += 'Matthew1471 <https://github.com/matthew1471[@Matthew1471]>;\n\n'

    # Document Settings.
    result += '// Document Settings:\n\n'

    # Set the autogenerated seciond IDs to be the GitHub format, so links work across both platforms.
    result += '// Set the ID Prefix and ID Separators to be consistent with GitHub so links work irrespective of rendering platform. (https://docs.asciidoctor.org/asciidoc/latest/sections/id-prefix-and-separator/)\n'
    result += ':idprefix:\n'
    result += ':idseparator: -\n\n'

    # This project uses JSON code highlighting by default.
    result += '// Any code blocks will be in JSON by default.\n'
    result += ':source-language: json\n\n'

    # This will convert the admonitions to be icons rather than text (both on GitHub and outside of it).
    result += 'ifndef::env-github[:icons: font]\n\n'

    result += '// Set the admonitions to have icons (Github Emojis) if rendered on GitHub (https://blog.mrhaki.com/2016/06/awesome-asciidoctor-using-admonition.html).\n'
    result += 'ifdef::env-github[]\n'
    result += ':status:\n'
    result += ':caution-caption: :fire:\n'
    result += ':important-caption: :exclamation:\n'
    result += ':note-caption: :paperclip:\n'
    result += ':tip-caption: :bulb:\n'
    result += ':warning-caption: :warning:\n'
    result += 'endif::[]\n\n'

    # The document's metadata.
    result += '// Document Variables:\n'
    result += ':release-version: 1.0\n'
    result += ':url-org: https://github.com/Matthew1471\n'
    result += ':url-repo: {url-org}/Enphase-API\n'
    result += ':url-contributors: {url-repo}/graphs/contributors\n\n'

    # Page Description.
    result += endpoint['description']['long'] + '\n'

    # Heading.
    result += '\n== Introduction\n\n'

    # Introduction.
    result += 'Enphase-API is an unofficial project providing an API wrapper and the documentation for Enphase(R)\'s products and services.\n\n'

    result += 'More details on the project are available from the link:' + ('../' * (file_depth + 1)) + 'README.adoc[project\'s homepage].\n'

    return result

def get_request_section(request_json, auth_required=False, file_depth=0):
    # Heading.
    result = '\n== Request\n\n'

    # Some IQ Gateway API requests now require authorisation.
    if auth_required:
        result += 'As of recent Gateway software versions this request requires a valid `sessionid` cookie obtained by link:' + ('../' * file_depth) + 'Auth/Check_JWT.adoc[Auth/Check_JWT].\n'

    # Sub Heading.
    result += '\n=== Request Querystring\n\n'

    # Table Header.
    result += '[cols="1,1,1,2", options="header"]\n'
    result += '|===\n'
    result += '|Name\n'
    result += '|Type\n'
    result += '|Values\n'
    result += '|Description\n\n'

    # Table Rows.
    for query_item in request_json:
        # Name (and whether it is optional).
        result += '|`' + query_item['name'] + '` ' + ('(Optional)' if 'optional' in query_item and query_item['optional'] else '') + '\n'

        # Type.
        result += '|' + query_item['type'] + '\n'

        # Value (or Type if not known) and a suggested option.
        result += '|' + (query_item['value'] if 'value' in query_item else query_item['type'])
        if query_item['type'] == 'Boolean':
            result += ' (e.g. `0` or `1`)'
        result += '\n'

        # Description.
        result += '|' + query_item['description'] + '\n\n'

    # End of Table.
    result += '|===\n'

    return result

def get_type_string(json_value):
    if type(json_value) in (int, float):
        return 'Number'
    # In Python, a bool is a sub-class of int.
    elif type(json_value) is bool:
        return 'Boolean'
    elif type(json_value) is str:
        return 'String'
    elif json_value is None:
        return 'Null'
    else:
        return 'Unknown'

def get_schema(json_object, table_name='.', field_map=None):
    # The fields in the current table.
    current_table_fields = {}

    # Store any discovered nested tables in this dictionary.
    child_tables = {}

    # A field_map contains all the table meta-data both static and dynamic. Does this table already exist in the field map? Get a reference to just this table's field_map outside the loop.
    current_table_field_map = (field_map.get(table_name) if field_map else None)

    # Take each key and value of the current table.
    for json_key, json_value in json_object.items():
        # Is this itself another object?
        if isinstance(json_value, dict):
            # Get a sensible name for this nested table (that preserves its JSON scope).
            child_table_name = (table_name + '.' if table_name and table_name != '.' else '') + json_key.capitalize()

            # This could return multiple tables if there are nested types.
            new_list_schema = get_schema(json_object=json_value, table_name=child_table_name, field_map=field_map)

            # Add the schema of the nested tables to child_tables (flattening the hierarchy).
            for child_table_key, child_table_value in new_list_schema.items():
                    child_tables[child_table_key] = child_table_value

            # Add the type of this key.
            current_table_fields[json_key] = {'type':'Object', 'value':'`' + child_table_name + '`'}

        # Is this a list of values?
        elif isinstance(json_value, list):
            # Are there any values and is the first value an object?
            if len(json_value) > 0 and isinstance(json_value[0], dict):

                # We can override some object names (and merge metadata).
                if current_table_field_map and (key_metadata := current_table_field_map.get(json_key)) and (value_name := key_metadata.get('value_name')):
                    # This name has been overridden.
                    child_table_name = value_name
                else:
                    # Get a sensible default name for this nested table (that preserves its JSON scope).
                    child_table_name = (table_name + '.' if table_name and table_name != '.' else '') + json_key.capitalize()

                # Take each of the items in the list and combine all the keys and their metadata.
                new_list_items = {}
                for list_item in json_value:
                    # This could return multiple tables if there are nested types.
                    new_list_schema = get_schema(json_object=list_item, table_name=child_table_name, field_map=field_map)

                    for child_table_key, child_table_value in new_list_schema.items():
                        # This is the current table we were updating.
                        if child_table_key == child_table_name:
                            new_list_items.update(new_list_schema[child_table_name])
                        # This is an extra nested table that was discovered.
                        else:
                            child_tables[child_table_key] = child_table_value

                # If this has been mapped/merged to a duplicate table name then we will need to append the existing dictionary.
                if child_table_name in child_tables:
                    # Get a reference to the existing list items.
                    old_list_items = child_tables[child_table_name]
                else:
                    old_list_items = None

                # Take each of the new list item keys.
                for new_list_item_key, new_list_item_value in new_list_items.items():
                    # Is this new key not present in all of the new items or not present in the old list of item keys (if applicable)?
                    if any(new_list_item_key not in item for item in json_value) or (old_list_items and new_list_item_key not in old_list_items):
                        # Mark this new key as optional (if it isn't already).
                        if not ('optional' in new_list_item_value and new_list_item_value['optional']):
                            new_list_item_value['optional'] = True

                # If this has been mapped to a duplicate then we will need to append the existing dictionary.
                if old_list_items:
                    # Take all the old list item keys.
                    for old_list_item_key, old_list_item_value in old_list_items.items():
                        # Is this old key not present in all of the new items?
                        if any(old_list_item_key not in item for item in json_value):
                            # Mark this old key as optional (if it isn't already).
                            if not ('optional' in old_list_item_value and old_list_item_value['optional']):
                                old_list_item_value['optional'] = True

                    # Merge the existing dictionary.
                    child_tables[child_table_name].update(new_list_items)
                else:
                    # Add the schema of this list of nested tables to child_tables.
                    child_tables[child_table_name] = new_list_items

                # Add the type of this key.
                current_table_fields[json_key] = {'type':'Array(Object)', 'value':'Array of `' + child_table_name + '`'}

            # This is just an array of standard JSON types.
            else:
                # Add the type of this key.
                current_table_fields[json_key] = {'type':'Array(' + get_type_string(json_value) + ')', 'value': 'Array of ' + get_type_string(json_value)}

        # This is just a standard JSON type.
        else:
            # Add the type of this key.
            current_table_fields[json_key] = {'type':get_type_string(json_value)}

    # Prepend this parent table (all its fields have been explored for nested objects).
    tables = {}
    tables[table_name] = current_table_fields
    tables.update(child_tables)

    return tables

def get_table_and_types_section(table_name, table, type_map):
    # Heading.
    result = '\n=== ' + ('`' + table_name + '` Object' if table_name and table_name != '.' else 'Root') + '\n\n'

    # Table Header.
    result += '[cols=\"1,1,1,2\", options=\"header\"]\n'
    result += '|===\n'
    result += '|Name\n'
    result += '|Type\n'
    result += '|Values\n'
    result += '|Description\n\n'

    # Any used custom types are collected then output after the table.
    used_custom_types = []

    # Table Rows.
    for field_name, field_metadata in table.items():
        # Field Name.
        result += '|`' + field_name + '`' + (' (Optional)' if 'optional' in field_metadata and field_metadata['optional'] else '') + '\n'

        # Field Type.
        if isinstance(field_metadata, dict) and 'type' in field_metadata:
            field_type = (field_metadata['type'] if 'type' in field_metadata else 'Unknown')
        else:
            field_type = 'Unknown'
        result += '|' + field_type + '\n'

        # Field Value.
        result += '|'
        if isinstance(field_metadata, dict) and 'value' in field_metadata:
            result += field_metadata['value']
        else:
            # Did the user provide further details about this string field in the field map?
            if field_type == 'String' and (value_name := field_metadata.get('value_name')):
                result += '`' + value_name + '`'

                # Add an example value if available.
                if value_name in type_map and len(type_map[value_name]) > 0:
                    result += ' (e.g. `' + type_map[value_name][0]['value'] + '`)'
            else:
                result += field_type

                # Did the user provide further details about this number field in the field map?
                if field_type == 'Number' and field_metadata.get('allow_negative') == False:
                    result += ' (> 0)'
                # Booleans will always be 0 or 1.
                elif field_type == 'Boolean':
                    result += ' (e.g. `0` or `1`)'

        result += '\n'

        # Field Description. Did the user provide further details about this field in the field map?
        result += '|'

        # Is "Description" one of the things the user has declared.
        if 'description' in field_metadata:
            # Add the description.
            result += field_metadata['description']

            # Is this a string or array that has a custom type?
            if field_type == 'String' and (field_value_name:= field_metadata.get('value_name')):
                # Update the description to mark the type.
                result += ' In the format `' + field_value_name + '`.'

                # We do not want to introduce duplicates.
                if field_value_name not in used_custom_types:
                    # Mark that we need to ouput this custom type after this table.
                    used_custom_types.append(field_value_name)
        else:
            # This field is not currently documented.
            result += '???'

        result += '\n\n'

    # End of Table.
    result += '|===\n'

    # Output any used custom types.
    if type_map:
        for used_custom_type in used_custom_types:
            # Check the custom_type is defined.
            if custom_type := type_map.get(used_custom_type):
                # Type Heading.
                result += '\n=== `' + used_custom_type + '` Types\n\n'

                # Type Table Header.
                result += '[cols=\"1,1,2\", options=\"header\"]\n'
                result += '|===\n'
                result += '|Value\n'
                result += '|Name\n'
                result += '|Description\n\n'

                # Type Table Rows.
                for current_field in custom_type:
                    # Field Value.
                    result += '|`' + current_field['value'] + '`' + ('?' if 'uncertain' in current_field else '') + '\n'

                    # Field Name.
                    result += '|' + current_field['name'] + '\n'

                    # Field Description.
                    result += '|' + current_field['description'] + '\n\n'

                # End of Table.
                result += '|===\n'

    return result

def get_example_section(uri, example_item, json_object):
    # Heading.
    result = '\n\n=== ' + example_item['name'] + '\n\n'

    # Example.
    result += '.GET */' + uri + ('?' + example_item['uri'] if 'uri' in example_item else '') + '* Response\n'
    result += '[source,json,subs="+quotes"]\n'
    result += '----\n'
    result += json.dumps(json_object) + '\n'
    result += '----'

    return result

def get_not_yet_documented():
    # Heading.
    result = '\n== Request & Response\n\n'

    # Placeholder Text.
    result += 'This has not yet been documented. Please check back later.\n'

    return result

def merge_dictionaries(a, b, path=None):
    "Recursively merges dictionary b into a.\nInspired by https://stackoverflow.com/questions/7204805/how-to-merge-dictionaries-of-dictionaries"

    # What path is currently being inspected.
    if path is None: path = []

    # Take each of the keys in dictionary b.
    for key in b:
        # Does the same key exist in a?
        if key in a:
            # Are both dictionaries?
            if isinstance(a[key], dict) and isinstance(b[key], dict):
                # Merge the child dictionaries by invoking this recursively.
                merge_dictionaries(a[key], b[key], path + [str(key)])
            # Are they otherwise the same values.
            elif a[key] == b[key]:
                pass # Same values do not require copying.
            # If a is a dictionary but b is a string then add the string to the dictionary as a description.
            elif isinstance(a[key], dict) and isinstance(b[key], str):
                # Set the description in the a dictionary to include the b string.
                a[key]['description'] = b[key]
            # If b is a dictionary but a is a string then add the string to the dictionary as a description.
            elif isinstance(b[key], dict) and isinstance(a[key], str):
                # Set the description in the b dictionary to include the a string.
                b[key]['description'] = a[key]

                # Replace the a value with the recently updated b dictionary.
                a[key] = b[key]
            # Do the types not otherwise match?
            else:
                # Error as data loss could occur.
                raise Exception('Conflict at %s' % '.'.join(path + [str(key)]))
        # It doesn't, so just add it.
        else:
            a[key] = b[key]

    return a

def main():
    # Output program banner.
    banner = 'Gateway Generate Documentation V' + str(VERSION) + '\n'
    hyphens = '-' * len(banner) + '\n'
    print(hyphens + banner + hyphens)

    # Load credentials.
    with open('configuration/credentials_token.json', mode='r', encoding='utf-8') as json_file:
        credentials = json.load(json_file)

    # Do we have a valid JSON Web Token (JWT) to be able to use the service?
    if not TEST_ONLY and not (credentials.get('token') or Authentication.check_token_valid(credentials['token'], credentials['gatewaySerialNumber'])):
        # It is not valid so clear it.
        raise ValueError('No or expired token.')

    # Did the user override the config or library default hostname to the Gateway?
    if credentials.get('host'):
        # Download and store the certificate from the gateway so all future requests are secure.
        if not os.path.exists('configuration/gateway.cer'): Gateway.trust_gateway(credentials['host'])

        # Get an instance of the Gateway API wrapper object (using the hostname specified in the config).
        gateway = Gateway(credentials['host'])
    else:
        # Download and store the certificate from the gateway so all future requests are secure.
        if not os.path.exists('configuration/gateway.cer'): Gateway.trust_gateway()

        # Get an instance of the Gateway API wrapper object (using the library default hostname).
        gateway = Gateway()

    # Are we able to login to the gateway?
    if TEST_ONLY or gateway.login(credentials['token']):
        # Load endpoints.
        with open('resources/API_Details.json', mode='r', encoding='utf-8') as json_file:
            endpoint_metadata = json.load(json_file)

        # Take each endpoint in the metadata.
        for key, endpoint in endpoint_metadata.items():

            # Skip if the endpoint is not meant to be documented.
            if not 'documentation' in endpoint:
                print('Warning : Skipping \'' + key + '\' due to lack of \'documentation\' filepath.')
                continue

            # This script currently exclusively writes "IQ Gateway API" documents.
            endpoint['documentation'] = 'IQ Gateway API/' + endpoint['documentation']

            # Count how many sub-folders this file will be under.
            file_depth = endpoint['documentation'].count('/')

            # Add the documentation header.
            output = get_header_section(endpoint=endpoint, file_depth=file_depth)

            # Check this documentation file supports making requests to the endpoint.
            if 'request' in endpoint:
                # Get a reference to the current endpoint's request details.
                endpoint_request = endpoint['request']

                # Does the endpoint support any request query strings?
                if 'query' in endpoint_request:
                    output += get_request_section(endpoint_request['query'], auth_required=True, file_depth=file_depth-1)

                # Get a reference to the current endpoint's response details.
                if 'response' in endpoint:
                    endpoint_response = endpoint['response']
                else:
                    endpoint_response = {'field_map' : {}}

                # Take each of the examples to learn the schema.
                if 'examples' in endpoint_request:
                    json_schema = {}
                    for example_item in endpoint_request['examples']:
                        # The user can supply the JSON to use instead of us directly querying for it.
                        if 'sample' in example_item:
                            # Extract the sample and use it as the response.
                            example_item['response'] = json.loads(example_item['sample'])
                        elif not TEST_ONLY:
                            # Perform a GET request on the resource.
                            print('Requesting example \'' + example_item['name'] + '\' for \'' + key + '\'.')
                            example_item['response'] = gateway.api_call('/' + endpoint_request['uri'] + ('?' + example_item['uri'] if 'uri' in example_item else ''))

                            # This variable can be inspected to hardcode a sample in API_Details.
                            debug_variable = json.dumps({ 'sample': json.dumps(example_item['response']) })[1:-1]
                            set_breakpoint_here = debug_variable
                        else:
                            print('Warning : Skipping example \'' + example_item['name'] + '\' for \'' + key + '\' as no sample JSON defined and TEST_ONLY is True.')
                            continue

                        # Get the schema recursively (we can override some known types, provide known value criteria and descriptions using the field_map).
                        json_schema.update(get_schema(json_object=example_item['response'], field_map=endpoint_response.get('field_map')))

                    # Merge the dictionaries with their nested values.
                    endpoint_response['field_map'] = merge_dictionaries(json_schema, endpoint_response['field_map'])

                # Ouput all the response tables.
                output += '\n== Response\n'

                # Add each of the tables from the derived json_schema.
                for table_name, table in endpoint_response['field_map'].items():
                    output += get_table_and_types_section(table_name=table_name, table=table, type_map=endpoint_response.get('type_map'))

                # Add the examples.
                if 'examples' in endpoint_request:
                    output += '\n'
                    output += '== Examples'

                    # There can be multiple examples for the same endpoint.
                    for example_item in endpoint_request['examples']:
                        # We cannot output an example without a response (either from querying the API earlier or hardcoding one).
                        if not 'response' in example_item:
                            print('Warning : Skipping example \'' + example_item['name'] + '\' for \'' + key + '\' due to lack of response.')
                            continue

                        # Take the obtained JSON as an example.
                        output += get_example_section(uri=endpoint_request['uri'], example_item=example_item, json_object=example_item['response'])
            else:
                # Add placeholder text.
                output += get_not_yet_documented()

            # Generate a suitable filename to store our documentation in.
            filename = 'output/' + endpoint['documentation']

            # Create any required sub-directories.
            os.makedirs(os.path.dirname(filename), exist_ok=True)

            # Write the output to the file.
            with open(filename, mode='w', encoding='utf-8') as text_file:
                text_file.write(output)
    else:
        # Let the user know why the program is exiting.
        raise ValueError('Unable to login to the gateway (bad, expired or missing token in credentials.json).')

# Launch the main method if invoked directly.
if __name__ == '__main__':
    main()